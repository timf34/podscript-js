# AI Alignment as a Solvable Problem | Leopold Aschenbrenner & Richard Hanania

**Podcast:** CSPI
**Date:** 9/2/2026
**Duration:** 1:01:35

---

## Speaker 1
[0:00] (instrumental music) Hi, everyone. Uh, welcome to the podcast. I'm here today with, uh, Leopold, uh, Eschenbrener. Uh, Leopold, why don't you tell the audience a little about yourself, your background, and what, what it is you do?

## Speaker 2
[0:12] Yeah. Um, no, originally I did economics research, uh, worked on long run economic growth, w- worked with the Global Priorities Institute at Oxford, um, and then worked on, um, basically doing a bunch of philanthropy in bio-security and AI safety with Future Fund and with s- sort of Open Philanthropy. And then recently have been, um, spending my time trying to get people to be more ambitious on making things happen on AI alignment.

## Speaker 1
[0:36] Yeah. And so you're young too, right? You're just, you're out of, you're just outta college, aren't ya?

## Speaker 2
[0:41] Um, I'm a couple years out of college, but I graduated college at, at 19.

## Speaker 1
[0:45] Mm-hmm. Yeah, and so we met at the Emergent Ventures gr- uh, conference a few years ago.

## Speaker 2
[0:49] Mm-hmm.

## Speaker 1
[0:49] And I was very, uh, impressed with your background. And then I see you, now you're popping up and you're writing, you know, very interesting things on, on AI. I didn't know actually, uh, bef- about the, uh, economic work. What, what have you done in economics exactly?

## Speaker 2
[1:02] Yeah, I mean, I, I actually, I wrote a, I wrote a paper on the relationship between long run economic growth and existential risk. Um, this is for just, like, econ theory. This is how, this is how I met Tyler initially actually, um, 'cause he sort of found that paper online and thought it was cool. Um, and then did some more work after on kind of like, um, you know, like long run economic growth and, and stagnation and things related to that.

## Speaker 1
[1:22] Yeah. So the, uh, let's see, we can start with the, we can, we can get to that stuff, that stuff's all interesting. Uh, let's start with the, uh, the AI stuff. I, you had a, um, article, um, called Nobody's on the Ball on AGI Alignment, and you know, this, I, uh, researching this topic and no- not a lot that's, like, surprising. It seems like a lot of people just sort of repeating a lot of the same arguments over and over again. Uh, what I really appreciated about your essay is it's a, um, you know, it's, it's either, you know, there's al- al- all, there's always either everyone's a doomer or they're, you know, don't worry about it. It, and I feel like what's, like, what's even the point? I mean, I've seen these things with f- uh, with, uh, Elizer who's going on all these podcasts and saying, "You're all dead. You're all dead. Let's start more. Let's start mourning." I mean, it's really, it's just not a, uh, sort of a, you know, he's just sort of he's adopted sort of a prophet persona and is just telling us, you know, the sky is falling. And then you have a lot of people who say it's no big deal and nothing's gonna happen. Uh, but you do something else. You say, "This is a problem, um, and it can be solved with, uh, with human in- ingenuity." So, can you talk about sort of, did, is that how you saw it? Did you, did you get frustrated sort of watching this debate and seeing nobody was sort of occupying the space?

## Speaker 2
[2:26] Yeah, I mean, I just, you know, especially recently, I mean, it's been awesome to see sort of people paying attention and, you know, uh, for a long time it's felt like there's been this sort of, only this sort of little bubble in ourself in the bay area. And now with ChatGBT, you know, all the online discourse is about it. But you know, all, there's sort of, like, so much online discourse about it and so much fretting. And you'd sort of think, "Oh, well, you know, I don't ..." People don't actually have it covered, basically. Like, people talk about, a lot about it, but not that many people are actually working on the problem. And I'm, I'm, I'm sort of like, my take on this, I think it's a very real problem, but there's sort of little bit too much kind of, like, strong word selling. And kinda, like, we need a little bit more sort of, like, ship rotating, actually looking at the problem. And you know, that, that sort of also kind of reflected my, my journey on the issue. Whereas, um, you know, for, for a while at the beginning I was kind of stuck in these, you know, sort of philosophical arguments with people. Like, you know, I don't know, you had one on, like, you know, with Hanson on your podcast where you'd kind of go back and forth and you'd do like, "Up and companies wouldn't deploy on safe systems." And you, you have all these sort of metaphors and whatever. Um, and the thing that sort of just, like, kind of made it visceral to me was like, I'm gonna look at the research work that is happening, you know? I'm gonna, like, look at these systems and look at the work in alignment. Um, and then I, when I was doing that, I was like, "Oh my god, this sucks." You know? It's like (laughs) it's just like there's barely anybody working on it. It's like literally, like, you know, you could've, like, basically are kind of, like, you can just kind of talk to the c- few dozen people who are actually kind of good on the issue. I mean, that's maybe even overstating it, you know? Um, and then, you know, the work very much, you know, if we continue sort of on that pace, which I very much hope we don't, and I think, I think we won't, but if we sort of didn't sort of get our act together more, I'm like, "Wow, we're really not on track to actually sort of do, do the alignment." You know? Um, and, um, but yeah, but also looking at it, I, I just think, I think we could be much more ambitious. I think there's sort of, you know, we can talk a bit about sort of technical things you could do or what the problem even is. Um, but I'm sort of like, uh, "Yeah, let's actually try to solve the problem." You know? Um-

## Speaker 1
[4:12] Yeah. And you give numbers on how many people are actually working on, uh, AG, uh, AGI alignment, right? And it's, it's not many.

## Speaker 2
[4:20] Yeah. No, no, I mean, there's, there's a survey that's kind of like, oh, maybe there's like 300 people working on it? And if you try to, you know, come up with a com- comparable figure for just kind of, like, overall MLAI researchers, that's maybe like 100,000 based on sort of how many people are attending conferences. Sort of like 300 versus 100,000, you know, that's, it's a pretty big ratio. I mean, you know, you could also kind of, like, narrow it down a bit more and just, like, look at the labs. And, you know, like, OpenAI has, like, 400 people. Um, and at least up until recently they had sort of all of seven people on their sort of, like, scalable alignment team, so the people working on this sort of long-term problem. So it's better than the overall ratio, but you know, still not that much. And it's not like they, um... yeah. And I mean, like, D- DeepMind, I mean, Google Brain th- had nobody, but now they've merged. But you know, so DeepMind that's, you know, maybe a couple thousand people, maybe have sort of like 20 people on it. And, you know, those, those are great 20 people or whatever, but they're not like their top scientists yet or whatever. Um, and so, um, hmm, I definitely, I was definitely kind of like... I thought, I thought, eh, you know, when I, when I initially encountered the discourse, you know, a few years ago, I was kind of like, "Eh, you know, this is, like, not neglected. Everyone's already talking about it." You know? It's sort of like, "I wanna work on something cool that nobody else is talking about," or whatever. And then you actually look at it and it's like, "Wow, nobody's actually doing anything." Like, you- you gotta actually do things people, you know? Um-

## Speaker 1
[5:36] Yeah. And so the, uh, and so when you count those few, a couple hundred people or whatever, is that including the people who are just, like, making it not say racial slurs and, and stuff like that?

## Speaker 2
[5:46] There's some overlap and it's sort of, um, hard to separate. I'm not, I'm not, I'm not counting sort of, like, pure ethics people. Um, and then this is sort of mostly trying to count people who are sort of trying to figure out this, like, long-term problem. Um, so, like, for example, at sort of OpenAI, you know, I said there's maybe just seven people who are doing scalable alignment. And then maybe there's another 10, 20 people who are doing the sort of more near term alignment stuff. Um-But yeah, I mean, this is, this is, this is, um, this is, um, you know, another sort of, another point, and I think this would be good to talk about is sort of the, um, uh, the sort of the long-term problem is separate from the near-term problem. And I think people kind of look at, you know, kind of like, quote unquote, "alignment" as in, like, don't, don't get ChatGPT to say bad words. And then they're kind of like scared away. I'm like, this is silly. You know, this is just the world kind of like, no, no, no, no. Um, this is, this is pretty different. Um...

## Speaker 1
[6:36] Yeah. And so, I mean, that, that's sort of, it's strange because, so where does all this money go? 'Cause my, my understanding of people concerned with AI alignment is it's like all these rich guys who have a lot of money and all these effective altruists, and...

## Speaker 2
[6:47] They're, they're not spending the money.

## Speaker 1
[6:49] They're just talking about-

## Speaker 2
[6:50] They're just-

## Speaker 1
[6:50] They're just tweeting about it.

## Speaker 2
[6:51] Yeah. I mean, look, like Open Phil, you know, this is basically the main funder. They're doing about 30 million a year on sort of technical AI alignment. Um, and even like, I don't know, like David Bao, I think is sort of like one of the best interpretability researchers, you know, people like that, they like either didn't get funding or barely got funding. So it's not like they're really like, it's not like there's sort of a massive amount of stuff happening. Um, I mean, there's sort of like, you know, more investment in some of the other cause areas. I mean, you know, most of Open Phil funding is still malaria nets basically. Um, and some in sort of like biosecurity stuff and in sort of, um, growing the EA community. So part of that is sort of trying to sort of get young people who will then later work on these issues. Um, but no, there really, there isn't, isn't that, there really isn't that much happening. And I think, and I think part of it is, you know, up until recently it was this sort of like totally sort of niche issue, you know, nobody was, nobody was talking about it. It seemed really weird. Um, and now people are paying attention and, you know, that's part of what, um, you know, I think it's a really, it's a really important moment now because I think the way we ultimately solve the problem is, is, you know, excellent ML researchers basically. I think it's an ML problem. You want excellent ML researchers working on it. And, um, now lots of sort of excellent ML researchers are getting interested and some of them are just like randomly DMing me on Twitter and things like that. Um, and so I think, yeah, I think, I think if I, you know, I think if I'm like a rich guy who, uh, who's sort of worried about this problem, I'm kind of like, no, there's plenty for you to do. And I think sort of now is the moment. Um, yeah.

## Speaker 1
[8:19] Yeah. And so, uh, so the, the Robin Hanson, you know, uh, reply to this is like, there isn't actually all that much for them to do. He gives this, uh, sort of metaphor if you, you know, took a peasant before the, uh, or whatever, even an intellectual before the, uh, uh, before the Industrial Revolution, and they were trying to say, you know, think about sort of the 20th century and how to, uh, you know, have arms control or, you know, how to have air traffic control or something like that. He's, you know, he says that there's basically not all that much to do, and you, you clearly disagree with that position. So could you explain why?

## Speaker 2
[8:51] Yeah. I mean, first I'll reply to Robin and then we can kind of go through what people are actually doing, but just sort of to the Robin thing, I mean, I, I'm, I'd be more sympathetic to that if I were, you know, 100% sure this was only happening in like 2070 or whatever. Um, I think it's, I mean, look, I think I believe in probability distributions, right? Um, and so I think it is very plausible that sort of all of this stuff doesn't go anywhere and it asymptotes and will hit sort of like, you know, will hit limits on how much we can spend and, you know, it'll be useful, but it'll be sort of like just another kind of useful tool. Um, I also just think it's plausible that, you know, really on sort of like fairly short time horizon, sort of like this decade basically, um, you know, we're gonna hit extremely, extremely powerful systems. Um, I mean, I think about the sort of jump from GPT-2 to GPT-4, right? GPT-2 was sort of like a toddler, right? GPT-2 could sort of count to five and then it would get confused, you know? It could have said, it could say kind of like vaguely plausible sentences and then it kind of veered off course. That was 2019, all right? GPT-4 was kind of like three, four years later. Um, I mean, and GPT-4 is pretty wild. I mean, I just, you know, encourage people to actually try to use it. And, you know, I f- people make the common mistake of sort of like using ChatGPT, but then like not paying the 20 bucks a month and then they, you know, they tweet out, you know, like, oh, ChatGPT can't do this thing. And then have the little green icon, which is, you know, GPT-3.5. I'm like, no, pay the 20 bucks a month and it's like, actually GPT-4 0000-

## Speaker 1
[10:07] Wait, you get a, you get a better version for 20 bucks a month? I didn't know that.

## Speaker 2
[10:09] Yeah, yeah, yeah. That's GPT-4. There's like, there was, there was a hilarious Ross Douthat tweet the other, the other day where he is like, "Look, the singularity will be nigh when ChatGPT can pronounce my name."

## Speaker 1
[10:17] Yeah.

## Speaker 2
[10:17] And he like tweeted it out and getting it wrong. And then it was like, no, if you pay the 20 bucks a month, it actually gets better.

## Speaker 1
[10:22] (laughs)

## Speaker 2
[10:22] So I replied to him, I was like, "Is your next column gonna be on, you know, the singularity is nigh now, Ross?" Anyway, so I think, you know, GPT-4 is, you know, it's, um, it's doing like really complex reasoning and, you know, you can sort of ask it to solve complex, you know, like math problems and it'll go through all the steps and, you know, it, uh, it, it can like, it can play chess. You know, it's like GPT-3 would only kind of like regurgitate plausible moves and now GPT-4 kind of like has learned to play chess and, you know, it's, um, it can... And I, I think there's sort of gonna be a lot more things that are, um, um, that we'll see once people kind of like hook up GPT-4 to more plug-ins and tools and, you know, I, I basically think GPT-4 is, is, you know, and it's like, you know, it's like 90th percentile at, you know, colle- you know, graduate and college exams, right? Which is wild. Again, GPT-2 to GPT-4 and that's like, you know, a few years. And so, look, just imagine a few more of those jumps and, you know, probably that won't be AGI, probably that'll be like a useful tool and will do everything. But I don't know, maybe it will. Um, and maybe it will. Um, and in particular, I think a sort of key, key thing is sort of like, can you... Do you get a system that can automate AI research itself? Um, and if you have a system that can automate AI research, then you could, that could sort of make many more algorithmic improvements and, um, you know, and then you get even more powerful systems. Um, anyway, all of that is to say I think it's like, Robin's like, "Oh, this is like, you know, in 500 years or whatever." I'm like, "No, I don't know. I think there's a pretty reasonable chance this is in five years." I'm like, "That's pretty soon. We should really be working on it."

## Speaker 1
[11:47] Yeah.

## Speaker 2
[11:47] Uh, yeah.

## Speaker 1
[11:49] Yeah. And I think to him, I mean, he would say, oh, well, he'd say a bunch of other things, but yeah, we could, uh, we could just sort of go into what do you think. So yeah, I like the-

## Speaker 2
[11:55] Yeah, yeah.

## Speaker 1
[11:55] ... probability, I like the probability sort of logic 'cause I don't know, like maybe Robin is right. Maybe even more likely than not. You, you are not-

## Speaker 2
[12:03] Totally.

## Speaker 1
[12:04] You are, um, yeah. So you're, you're not, uh, you're not working on this with certainty that people are gonna die. Like you're, you're, your work-... could be useless to humanity, right? There's a po- probability-

## Speaker 2
[12:16] Sure, sure.

## Speaker 1
[12:16] ... there's really nothing to worry about, right? Maybe some kind of problem. And I don't-

## Speaker 2
[12:20] Well, maybe, maybe even t- maybe even the median case. I mean, I just think something like 10% is incredibly high. I mean, it's like, this is sort of, it's some- you know, I think, I think most people, most people just have sort of like screwed up probabilities all across the board. There's like, you know, 10% chance climate change will kill everybody. And it's like, no, there's basically a 0% chance climate change-

## Speaker 1
[12:36] Yeah.

## Speaker 2
[12:36] ... will kill everybody. It's just like 10% chance is actually very high. Um, yeah.

## Speaker 1
[12:40] Yeah. And people don't think, people don't think like that. No, I mean, the vast majority of people would not work on an issue where the immediate outcome was they're- they're doing nothing, right? That- that- that I think no- very few people would swallow that. Um-

## Speaker 2
[12:51] I mean, in pract- I don't know. People, most people don't do anything, you know? But-

## Speaker 1
[12:54] Most people don't do anything, period. (laughs)

## Speaker 2
[12:58] (laughs)

## Speaker 1
[12:58] That's true. That's very true. Yeah, right.

## Speaker 2
[12:59] Um, uh, yeah. I mean, I think, I think 10% makes it sort of like definitely the biggest deal or whatever in the world, like the most important problem. And it's like just the most important problem right now.

## Speaker 1
[13:08] Yeah.

## Speaker 2
[13:08] Uh-

## Speaker 1
[13:09] Are you impressed with those like expert surveys? 'Cause they do these expert surveys and they're like, okay, 50, they know the experts say 10, 15% it's gonna kill everyone. Wow, that's amazing. I think that people are not good with probabilities. If you went to nuclear, like, uh, war experts, you say, "What's the odds we're gonna all die of nuclear war?" They'd probably say 10%. If you went to, I don't know, pandemic people, they'd probably say 10 ... y- there's probably 50 things-

## Speaker 2
[13:29] Yeah, yeah, I mean-

## Speaker 1
[13:29] ... people will give a 10% probability for.

## Speaker 2
[13:33] Yeah, I mean, look, here's my take on this. I think, one, there's sort of these like really broad kind of like surveys or forecasting stuff are just like pretty generally low signal. So another example of this is like superforecasters. And I think The Economist was even citing, oh, the superforecasters gave some low probability. But we had- we had actually commissioned superforecasters a while back, and we commissioned two different sets. And one of them was the one The Economist published, and another one just gave like much higher numbers, basically numbers like the ones I'm giving, you know, in the sort of like 5 to 25% range. I think there's some reason why we weren't supposed to publish them, so I don't have the exact number for you. But basically there's sort of like two sets, both reputable superforecasters and they give like wildly divergent numbers. So I'm sort of like, I don't know, just kind of like low signal there. Um, but yeah, I mean, I do think it's significant. I- I will say I think it's significant that, um, you know, sort of like, you know, like real AI people are talking about this, or just like basically sort of the people who are really concerned about this are the AI lab CEOs. And I think there's some sort of like common causal connection, which is like the AI lab CEOs, like, you know, three years ago already realized that like AI progress would be really rapid and would be a really big deal. You know, when people like Tyler Cowen were like, "Ah, I don't know, won't be," or whatever. And it's like, no, the AI lab CEOs were right. They're sort of close to the thing, they saw it happening. And the same AI lab CEOs are really worried about the AI alignment stuff, because they're like, they're very close to the progress on the AI alignment stuff and they're like, "Wow, I'm just like not impressed by the progress right now." And it's like, we really got to up our game there. Um, and, um, yeah, so I sort of ... I think that's worth taking seriously.

## Speaker 1
[14:55] You know one thing that AI is not good at? This is a bit of a digression, uh, Leopold, but the AI is not as g- not good at making transcripts of me talking. It can make good tr- and I- I- I suspect it wouldn't make good transcripts of you talking, because I've tried to do automated AI, and it really depends on the guest. Some people speak in like complete sentences and sort of like the way people write. Me and you, I think are different. We sort of speak with false starts back. Marc Andreessen, he's got a podcast. You couldn't do, you couldn't do it automated with Marc Andreessen either. So what- what AI can do a-

## Speaker 2
[15:23] Huh.

## Speaker 1
[15:23] What AI could do a transcript -

## Speaker 2
[15:24] They'll do it this year, you know? It's- it's like one of these things. Like, you know, Bryan Caplan, that was glorious. Bryan Caplan was like, you know, ChatGBT got a D on my midterm or whatever, and you know, I- I bet ... won't do it this decade. And then literally like three months later, you know, it's like GPT-4. It's again the one you pay 20 bucks a month for came out, and he was like, "Oh, well, actually like got the best score in my class." Um, yeah.

## Speaker 1
[15:45] The, uh ... Okay, so I haven't checked in like, you know, six months or something. So maybe- maybe it can actually do my voice by now. Um, it does, uh, it does go fast. The, um, well the- the- the Chit Chat-GP, the 20 bucks you pay for, it's just the thing, it's just the input/output. You type stuff and it comes back. I can't, like, give it an audio file and say, "Make a transcript." Can I?

## Speaker 2
[16:02] I mean, uh, not yet, but-

## Speaker 1
[16:05] Yeah, the plug-ins, the-

## Speaker 2
[16:06] ... pretty soon, yeah.

## Speaker 1
[16:06] ... plug-ins will come.

## Speaker 2
[16:06] Yeah, yeah, because, you know, the transcript, the transcript is, you know, wh- you know, Whisper or whatever. They're sort of other AI models that'll do it, and you know, Chat- you know, ChatGBT is getting a code interpreter. So yeah, probably, I mean, maybe like later this year, you know, you can just sort of upload it and they'll do it.

## Speaker 1
[16:18] Yeah.

## Speaker 2
[16:18] Anyways, so let's ... You wanna talk about the- the sort of like technical problem of alignment?

## Speaker 1
[16:22] Yes, I- I do. Um, so yeah, there's a different- couple of different ways. I mean, you just, you, the- just talk about whatever you think is sort of the most important points to them.

## Speaker 2
[16:29] Yeah, yeah. Sounds good. All right. So look, what is- what is alignment? I feel like this has sort of become a corrupted term or whatever. And I think at a basic level, it's just, look, you want the system to do what it says on the tin. And it's like, look, I want to ask this ... For example, I think a good example is just like you wanted to be honest, right? I ask a system a question and I want to like honestly report what it knows. Um, and so I think forget about, you know, some people are like, oh, complex human values or these weird an- forget about that. There's just like sort of like get the system to do what it says on the tin. Um, um, yeah, so how do we do that? Um, the basic method we use right now is basically human supervision. Um, so we're kind of like, um, you know, we have the system. So first we do this big pre-training, we have it read the internet or whatever, and then, you know, the system ... It's sort of like sort of a, you know, a- a child that has like watched a bunch of people like speak or whatever and it's learned the- learned the language. But then we're like, okay, now we want the system to do things like ChatGBT, and then how do we do that? We sort of just train it with sort of human thumbs up, thumbs down. So we're like, give us an answer, we're like, this is a good answer, this is a bad answer. Um, and then, um, it learns from that and it produces answers that sort of get thumbs up. Uh, and you know, you could sort of imagine other versions of this that are like, look, we want, um ... Yeah, anyway, so that's- that's the basic thing. It's RLHF, it's human supervision. Um, the sort of key issue for this long-term problem is, um, how do we supervise a superhuman system, right? So right now, ChatGBT gives an answer, we can sort of easily tell if it's sort of dumb or not, or if it's, you know, honest or not, and we're like thumbs up, thumbs down. GPT-7, you know, maybe we have GPT-7 and it's like basically superhuman at a bunch of things and we hook it up to, you know, a bunch of different ... The internet and plug-ins and whatever, and you're basically having it run our business. And then we're like, look, GPT-7, like give- give me a business plan or like write me a new software to improve my business. And then it, you know, gives us, you know, 10,000 pages and really complicated software program. And then we're like, uh, you know, I mean, does this like violate any laws? Is this okay? Um, and like we don't know. It's like way too complex for us to sort of, uh, supervise. Um, and so sort of the basic method we're using doesn't work anymore. Um-... human supervision doesn't really scale to superhuman systems. Um, we can't even sort of detect the bad behavior, uh, so we can't prevent it. Um, and so that's sort of the basic situation we're in, and like, how do you fix that? Um, and there's a bunch of different things people are doing.

## Speaker 1
[18:54] Yeah. Okay. So let me ask you this. Would the, would the honesty ... If you solved honesty, would that solve everything, if you just had a thing that never lied to you? Um, would you, would that be able like i- it couldn't manipulate you, right? It could, but it could like try to convince you with ?

## Speaker 2
[19:08] So, yeah. I think, I think honesty would get you pretty far. I'm not sure if it would solve anything. But basically, I think if you get, you know, GPT-7 to be honest with you, I think, yeah, I think it'd get you pretty far. I think s- I think honesty is great, because I think you can kind of think about it. It makes a lot of sense. Sort of like obvious you want it. I think it's also just surprisingly hard. Um, there's, there was actually a paper yesterday that kind of illustrates this. So, you know, sometimes, y- yeah, I'll, I'll talk about the paper briefly. So, you know, you think the model g-, you know, sometimes, sometimes the models do chain of thought, right? So you're kind of like, you know, you give it a, you give an exam question, and you go, "Here are, here are A, B, C and D." And then, um, you know, "Tell me which one is correct, and explain your thinking." And then, it'll choose B, and be like, "Look, it's B, because you know, first step in this math problem is this. Then, we do the second step," and whatever. And so, you know, you could imagine like, "Oh, man. This, this will make, make things fine, because well, we can like look at what these models are thinking, and they'll be honest, and it'll be totally fine," right? Mm. But in fact, sort of what the model is thinking can be quite different from what it says. And so, in this paper, basically, they kind of like, um, got the model to choose a, a different, a different answer for a particular reason. Um, in this case, it was for ex- an example of this was they sort of gave it a bunch of examples where the answer was always A. So they give it a test. The answer was always A. Um, and then, uh, learned that, and then it sort of would answer A. But then, it would give, make up some sort of totally different reason for why it chose A. So it'd be like, "Oh, the answer's A, you know, because step one is that, and then step two is that, and step three is that." Um, where in fact the reason it chose A is 'cause every other answer was A. But it didn't say that, right? Um, and it, in fact, knew that it was wrong, um, sort of on the fact. Mm. And so, um, you know, on, or on the human supervision side, you know, just one thing, one way to think about why honesty might be hard is, um, you know, um, uh, m- will the model learn to sort of tell us the truth, or will-

## Speaker 1
[20:57] Is this, is this a thing of like self-aware ... Is this like a self-awareness thing? Like, it doesn't even know like what it's doing, right?

## Speaker 2
[21:02] No. No. No. It, it, it doesn't, it doesn't, it doesn't necessarily know what it's doing.

## Speaker 1
[21:06] Yeah.

## Speaker 2
[21:07] It can't be self-aware.

## Speaker 1
[21:07] So it can't be honest with you. It has to know to be honest. So that's like a, a k- sk- You've trained it to s- say A, you're saying, but then it gives a false reason for saying A, right, just because it doesn't have like a self-awareness? Is that the idea?

## Speaker 2
[21:21] I mean, basically, it's just like the model will do what you train it to do, but it sort of ... that might behave in weird ways. For example, if you just train it with sort of thumbs up, thumbs down, the thing it might learn is not be honest, but sort of like say what a human would think. Right? So if you'd trained an LLM in like the 1300s, the model would definitely say, "God exists," uh, because like all the answers were, "Deny his god." It's like, and people are like, "Thumbs down. Thumbs down." And then, the model sort of generalizes, "Oh, I should say the thing the humans want to hear," rather than kind of like, "You know, actually go- you know, this is, this is totally wrong. You know, this is some weird "

## Speaker 1
[21:51] Yeah.

## Speaker 2
[21:51] "... religion "

## Speaker 3
[21:51] Yeah.

## Speaker 2
[21:51] "... you get data."

## Speaker 1
[21:51] And this is the problem. This is just the problem with re- reinforcement learning. You don't know what it's picking up. It's just sort of doing stuff. Right? And so, this is the, the, is the, the algorithm ... I mean, is it, is it that simple? The algorithm is basically that you tell it to move towards, uh, well, move towards A, and move away from B, and that's basically just all that's going on? And is that what they t- what they refer to as like the black box, and you really don't ... you can't really see what's going on at that point?

## Speaker 2
[22:14] Yeah. Yeah. Totally. So you know, you're, you're basically ... Yeah. You're, you're training it through some sort of process of evolution. You're giving it sort of some thumbs up, thumbs down signal. Um, and, um, you know, and that, that might work really well. And there's sort of some, just some question of how it generalizes. So you know, like in the thumbs up, thumbs down case and the honesty case, you know, um, does it generalize? Yeah. Does it generalize to tell you the truth, or does it generalize to sort of tell you what, what it thinks the humans want to hear? Um, um, and, um, you know, and there might be all sorts of things you can do. It's like, "Oh. This is so simple. We'll just like get it to explain its reasoning." But in fact, you know, what it's saying is not what it's actually reasoning. It's just some other thing it made up. And in fact, it's reasoning with some totally different things. Um, and so sort of even honesty is just like quite hard. I think it's like very much solvable, and so there's some like cool stuff you can do, and we can talk about that. Um, but it's ... you don't really get it by default. Um-

## Speaker 1
[23:01] Okay, so how, how is honesty ... I mean, then might as well talk about it now. How is honesty solvable?

## Speaker 2
[23:07] I mean, so it's not solved, but I think there's things you can do. So for example, I think there's a, a recent cool paper by Colin Burns, um, that basically, you know, looked inside the model and found sort of like little, like truth neurons, um, and sort of simplified. But basically, you know, looked inside the model, and um, yeah, found a little lie detector. Um, and then, what's particularly cool about this is sort of he found the lie detector in an unsupervised way. So it wasn't like he gave it a bunch of examples. It was just basically based on some sort of like consistency properties that he was able to like identify this sort of like, these truth neurons. Um, and that's really important, because unsupervised methods sort of don't have this problem of sort of human supervision, where they would predictably fail at superhuman systems. Um, and so, you know, like sort of example where you could use this sort of like truth neuron thing, or like what this thing accomplished was, you know, you can sort of prompt a model to tell you wrong things, right? You can like ... You can prompt a model with kind of like, you know, Reddit falsehoods or whatever, and then you could ask it, "What's the capital of Pennsylvania?" And it'll just kind of say, um, "Philadelphia that's the common misconception," when in fact the capital is Harrisburg. Um, and the model will say that. But in fact, if you kind of like look at the truth neurons, it's like the truth neuron is like, "No, no, no. I'm, I'm, I know this is incorrect." Um, and um, and yeah, and I think this is, um ... I think there's like two parts of this that are kind of cool and gesturing at, um, uh, directions that I'm excited about. You know, part one is basically sort of like looking at model internals. You know, so this broader thing of sort of interpretability, um, and it's sort of a key advantage we have, um, on these models basically is we can look inside of them. Um-... and then part two that I think is really exciting about this, this particular paper is, it's sort of unsupervised. So, you know, I was saying, you know, human supervision is what we do right now, but it might predictably fail with superhuman models. So are there sort of things we can do that are unsupervised, or that are sort of much weaker human supervision, um, and that still work?

## Speaker 1
[25:02] Yeah. That's interesting. So, uh, is it the, um, uh, so the truth neuron, right? So it, it must have some, how does it know that, uh, Harrisburg is... So it, ha, ha- must have some conception, it sounds like, of credible sources of information and non-credible sources of information. So it looks at, maybe, at Encyclopedia or something, and it sees, "What capital of Philadelphia?" And then it sees some dumb person on Reddit and it's, you know, they say Philadelphia. Um, is, is, is something like that going on? How does, how does it, like, determine truth in the, uh, in these models?

## Speaker 2
[25:34] I don't know, the same way you determine truth, you know? It's sort of like, you've, you've read a bunch of things and you've learned things and you have a sense of sort of what information's true and what information is false. And, you know, there might be certain situations in which you kind of like lie or say the wrong thing because for whatever reason, but you kind of still know whether it's true or false, and there's some sort of, like, separate way in which your brain registers that. And, um, you know, sort of the column papers basically saying, "Oh, these models do that too." And we can, we can sort of like locate other things in these, these models' brains, which is, you know, cool. So, like, I think there's another, another one of the sort of cool interpretability papers is one by sort of David Bau's lab. And they basically, like, found or like specific pieces of facts were located in their brain. Um, and then they were able to change it. So they're like, "The Eiffel Tower is in Paris." Right? And so it's like, okay, it's in Paris. The model knows it's in Paris. You can say, "The Eiffel Tower is..." and the model will say, "In Paris." Um, and then they're sort of able to go into the head basically and, and sort of find, find the parts of the model that corresponds to that factual knowledge of, "In Paris." And then they were g- able to do even more. They're able to change that. So they're like, "Okay, now we're gonna make it in Rome." So they sort of, they changed that part of the model and now it's like, "Now the Eiffel Tower is in Rome." And now when you ask the model questions about the Eiffel Tower, the model thinks it's in Rome. Um, so you're like, you know, "How do I get from Berlin to the Eiffel Tower by train?" And the model will give you train directions to Rome 'cause it now thinks really, like, it really thinks the model's in Rome. You sort of found the core source. Um, and, uh, you know, anyway, so this sort of work is super embryonic right now. Um, and, you know, bare- you know, barely any work, um, but I, I think, I think there's a lot of potential there. Um, and in particular, I mean, one thing I'll just say, you know, about sort of broadly what is the hope for AI alignment is, the same way we'll be able to use AIs to automate sort of AI capabilities research and build more powerful AI systems. Hopefully we're gonna be able to use AIs to, um, automate the sort of like AI safety or AI alignment research. So maybe we can use AIs to sort of help automate interpretability research. And in fact, just yesterday there was sort of a paper that came out from OpenAI where they did sort of embryonic steps towards automating interpretability

## Speaker 1
[27:36] Yeah. So on your, uh ... So j- j- like a, a simple question. So when you say, go into the machine and see where it, you know, believes where the Eiffel Tower is, you'll be, uh, it's, you're all doing this by head. You're, you're prompting stuff and you're ex- extracting, right?

## Speaker 2
[27:50] Yeah. You're, you're looking at model activations basically. So it's like, you know, when, when, when you, when you give, you know, when you ask ChatGP- you know, when you ask one of these language models, you know, um, "What is the capital of Pennsylvania?" Or whatever, then, you know, then there's, you know, GPT-3 or whatever has 175 billion parameters. And so there's just a whole bunch, billions of neurons that are sort of like lighting up in different ways. And then you kind of like look at the pattern of how things are lighting up. And then you're like, "Oh God, where, where's, where's the knowledge?" You know? Uh, but there's sort of better ways of doing that. Um, it's the basic-

## Speaker 1
[28:21] But what's the, just for not, not computer science audience, the, uh, lighting up means what? It, there's not something that's lighting up. It's not like Christmas lights, right? What are, what are you, what are you looking at when you're investigating this stuff?

## Speaker 2
[28:31] Oh, I mean, so, I mean, literally neural networks are just like, you know, GPT-3 is like 175 billion numbers, sort of parameters. And, um, you know, you put the words in. The words are also just sort of translated into numbers. You put the words in, they're numbers. And then there's like, they're multiplied and added in a bunch of different ways. And then a couple numbers come out, and those are translated back into words. And then the way these models are literally trained is you just kind of like turn the knobs on the 175 billion parameters again and again and again, while you're kind of like reading the internet to like kind of generate little better and better, better sort of text at the other end. Um, until it like, actually is like, wow, this, this thing is actually really good. Um, it's, it's pretty magical. But this, I mean, this is part of the black box thing, right? It's like there's no sort of like, there's no like computer code you're writing. You're just kind of like, you're specifying the training process. You're specifying this almost kind of like evolutionary process. You're kind of like, look, read the internet and be able to kind of like predict the next word. Um, that's sort of the pre-training. We're gonna adjust the knobs so you're good at that. And then after we're gonna adjust the knobs a little bit more so that it gets good, get thumbs up rather than thumbs downs from humans. And that's, that's the process.

## Speaker 1
[29:36] So it's basically, I mean, it's an, uh, it's an algorithm, right? That's f- that's fundamentally what we have.

## Speaker 2
[29:42] Mm-hmm.

## Speaker 1
[29:43] It's not a, um... But is there... When, when, like, when they say black box, is it that like, it's just too much? Like is, is there like, like is it just like too much information there? Or is it like literally this information cannot be extracted any way?

## Speaker 2
[30:02] Um, it's just, um... So I mean, you can look at the activations basically. You can look at the little neurons lighting up. It's just the same way sort of like a human brain use... Yeah, it's pretty similar to a human brain. It's sort of like, it's basically like the information you have is basically if you could sort of like scan all the neurons in the human brain. So it's more information than we have for human brain, but we still don't know what the heck all the neurons are doing. It's just basically this like maze of numbers. And this maze of numbers was just kind of like came out of this like, these giant computers that were optimizing the network, um, in this sort of basically evolutionary training process.

## Speaker 1
[30:31] Yeah. I got it.

## Speaker 2
[30:33] Um...

## Speaker 1
[30:33] S- okay. So you got, so sort of, like, the intelligence is sort of like an emergent property of, of our brain? Is, is that how-

## Speaker 2
[30:38] Yeah.

## Speaker 1
[30:38] ... how it is? But you can look at the neurons and you can look at the numbers.

## Speaker 2
[30:42] Yeah.

## Speaker 1
[30:42] You can look at the literal numbers.

## Speaker 2
[30:43] Yeah. Yeah.

## Speaker 1
[30:44] Binary, uh-

## Speaker 2
[30:44] Like the literal activations. Yeah.

## Speaker 1
[30:45] ... zeros and ones.

## Speaker 2
[30:47] Yeah. Basically, yeah.

## Speaker 1
[30:48] Uh-huh, okay. Yeah. That is, that's a, that's interesting. So you're, uh, so...Uh, the, in the, in the, um, article on nobody's on the ball, you talked about reinf- uh, fo- uh, reinforcement learning from human feedback. So that's thumbs up, thumbs down. And then you talk about sort of the, uh, trying to iter- iteratively make it work, uh, the scalable oversight. So can you talk about that a little bit?

## Speaker 2
[31:08] Yeah. Yeah. Sure. So yeah, maybe I should, maybe I should give sort of a bit of sort of a broader overview. So yeah, so baseline basically the thing we're doing right now is sort of like just human supervision. So we tell it whether it's good or bad, or we tell it whether it's honest or not. Um, and that's how we get ChatGPT to not say bad words. And that, that works pretty well. I mean there's lots of sort of engineering work to make it sort of even better, but baseline it works. And the key difficulty is we're gonna maybe build superhuman models and human supervision might not work anymore. And so, okay, so how do we sort of like get something that scales to sort of superhuman models? Um, and, um, so what is there? I mean one, there's just kind of like MIRI and like Eliezer who've been kind of around for kind of like 20 years or whatever, basically just philosophers. And that, that basically I just kind of want to say is like forget about all that or just like don't, don't worry about that. It's like, th- I don't, it's not gonna lead anywhere. I think the way, the way we solve this problem is sort of, you know, ML research. Um, and, you know, shape rotating not word selling. And I think it's actually sort of done some damage to the field where people think it's some weird philosophy thing rather than sort of like real technical problem. All right. So what else can you do? I think the sort of like baseline lab plan is basically, all right, so we'll do the human supervision thing, the sort of RLHF, thumbs up, thumbs down. And then we're gonna build human level AI. Um, and then once we have human level AI, the AIs are gonna do AI research and we're gonna get even more powerful systems. But we're also gonna get the AIs to do alignment research. Um, and, um, hopefully they'll solve it. And so I don't know. That, that might work. Seems, seems like that's, that's one of the reasons I'm like, yeah, median scenario's maybe okay. It's like maybe we can also just get the AIs to do the alignment research and like, you know... Anyway, I, I think, I think it'd be good to prepare that a bit better. But you know, like one of the things is AIs doing interpretability research like I mentioned earlier. But, um, so that might work. One particular component of this plan is sort of what people call scalable oversight. Um, so it's basically like tr- tr- try to sort of augment the human feedback to be a bit better. So for example, you know, you're, I mentioned earlier this scenario of like look, you're faced with this sort of like 10,000 page plan with 100,000 lines of code and you're like, "Ah, I can't evaluate this." And, um, maybe one thing you could do is you could have little AI helpers that help you evaluate it. So you're kind of like, "Oh, uh, do I, do I give this a thumbs up, thumbs down?" And I'm like, "Oh, well I have all my little AI helpers." Um, and maybe I'll even have the AI helpers debate each other, um, and so on. But basically you kind of like use AIs to help you better supervise other AIs. Um, anyway and, and th- roughly I call this sort of like the iterative plan. So you kind of like, you go as far as you can with human supervision and then you use AIs to like help, help your supervision, amplify that supervision, use AIs to help you do alignment research. Um, and yeah, anyway, it might work, it might not work. I'm like, I'm, I think this is like one of the great hopes. But I just like, I am, I feel really uneasy as just kind of like relying on only that. Um, yeah.

## Speaker 1
[33:50] Sounds like the same a- the sa- sounds like the same alignment problem, right? Who monitors the monitor, right? If the, if the, you know, you're not aligned with the, the hall monitor AI, right? It's the same problem.

## Speaker 2
[34:03] I mean that's one of the reasons it might fail. Yeah.

## Speaker 1
[34:05] (laughs) Okay. What e- what e- what else is there? Is there, is there anything else besides? Is, do we have any other-

## Speaker 2
[34:10] Yeah.

## Speaker 1
[34:10] ... ideas

## Speaker 4
[34:10] Mm-hmm.

## Speaker 2
[34:10] So I mean then there's sort of some of the interpretability work I've described. I mean so the interpretability work I've described so far is a bit more kind of like the sort of like top down interpretability. Uh, most of the time when people talk about interpretability they mean mechanistic interpretability. So that's basically we're gonna like sort of like think of this as sort of like the basic physics version of interpretability. So we're gonna sort of like understand these networks from the ground up. Uh, Chris Olah at Anthropic has done good, you know, sort of the pioneer of this and has done awesome work. Um, Neel Nanda is a person who sort of maybe you've seen sort of online and is active and has done some really interesting work on this. Mm, for example he's done some work on sort of like grokking. Basically sort of sometimes it seems like neural networks suddenly understand a thing and what's going on there. Um, and I'm, I'm like very excited that people are doing this work and I think it might be really helpful. I think, um, you know, I'm a little bit worried that it's, uh, way too hard of a problem or it's just sort of like, um, you know, y- y- yeah. It's sort of like maybe the AGI is coming just a few years and you're kind of like, you know, maybe we're gonna like about to turn on the nuclear reactor basically and we're like, "Oh, is the nuclear reactor gonna be safe? How can we do the safety engineering for the nuclear reactor?" And then the people over here and like, "Ooh, I'm gonna do work with particle colliders and we're gonna do some basic physics and, you know, that's gonna make our nuclear reactor safe." And I'm like, "Ah, maybe. I'm glad you're doing your thing." But, um, and so anyway, the sort of interpretability work I'm a little bit more excited about is some of the work I mentioned earlier which is basically, um, you know, more targeted. So it's like, you know, instead of trying to sort of understand from the ground up, we're gonna just kind of like try to look for the thing that's like is it telling the truth or not. Um, and maybe even try to... Yeah. Um, or the sort of David Bao one which is like look, try to look for specific facts and maybe later we get a better... We want to like look for specific things. Um, we want to maybe change what they're trying to do or whatever. Um, yeah. All right. So that's interpretability. One more frame I'll mention that I'm sort of pretty excited for how to think about this and this is newer and there isn't that much online about this yet. But this is also, um, you know, from some of the OpenAI people, Colin, uh, Burns at OpenAI in particular, um, is um, you can think about this problem as a, as a generalization problem. Um, and, um, in particular a sort of easy to hard generalization problem. So there's a bunch of examples where humans can supervise it and those are sort of easy examples where, um, humans understand what's going on. And then there's sort of like hard examples that, um, humans no longer understand. And the question is how does, how does sort of like what you tell the AI generalize? Um, and so generalization is basically a core ML problem or it's, you know, one of the reasons deep learning is so successful is because, um, uh, neural networks and deep learning are so, so good at generalization. Um, and you, you know, there's sort of very basic generalization problems which is like you showed a bunch of pictures of birds and you wanted to learn to look at the sort of like the bird itself and not the blue sky behind it. And a sort of-... naive failure mode is like, oh, it'll learn to think bird when it looks at a blue sky. Um, and, um, you know, that, that's called spurious cues. And, you know, people are able to solve that sort of... And it's just sort of basically an engineering problem. Um, and so same, same thing here is, you know, like when you're giving it thumbs up, for example, for honesty, is it gonna generalize to kind of like tell the truth or is it gonna generalize to, um, um, you know, what would the human think? And so you wanna make sure it generalizes correctly. And, and there's a bunch of sort of very simple settings in which you can study this. Um, yeah, maybe there's sort of too much detail. But basically, you can kind of look at like, um, you know, very normal ML models and be like, how does this generalize if we sort of only give it sort of easy labels and we don't give it all the true labels? Um, and, um, that's pretty analogous to the ultimate problem. Um, and it's sort of pretty exciting to me if we can just kind of like make this a science. We're gonna study generalization. We're gonna understand generalization really well. Um, and, uh, we're gonna study a bunch of settings and test beds that look similar to the ultimate problem.

## Speaker 5
[37:59] Yeah. This is, uh, this is hopeful. I mean, there's a, um, uh, yeah, there's a, um... Like it's, it's... When you think of just sort of I think when most people talk about this, it's like, oh, there's gonna be this thing that wakes up one day and then it's gonna be plotting against us and, you know, and just like a human would and it's gonna do all these other things. It's just gonna be super intelligent. And what you're saying is like we can sort of... We can break this down into a bunch of manageable parts, make it into something of a science. Um, and then, you know, it'll be something that'll be very advanced in some ways and like, you know, we'll, we'll retard some things. Like, we'll retard its ability to, like, you know, lie to us, right? Hopefully. Or we'll r- we'll retard this or it's like, you know, maliciousness or (laughs) it's blood lust or whatever, right? It's like we can, we can work on all these things. And instead of seeing it as, like, this god that just comes up, like it's gonna be... It's coming here through the process, right? There's... It's not gonna be a thing that's just gonna have like a one million IQ and then otherwise be a blank slate, right? Is that... That's where we Totally, yeah. I mean, I think, I think it could happen a lot faster than people think, you know. And sort of I think, I think, like I said, I think we get the sort of like human level thing the next, in this decade basically. And I think once we get the human level thing, I think it'll start getting really good at AI research and make a bunch of algorithmic progress. And I think we might get like much more advanced systems really quickly. Um, but even though all this is happening really quickly, I just, yeah, I think there's tons of things we can do. Um, and, um, yeah, and in particular, I just think it's like, again, it's sort of like an ML problem. Um, that said, I just think, I think there's a very reasonable chance we just like fail and we just like don't e-... Like, I mean, in particular right now, we're like not even trying that much. And so it's like first step, we really got to try, you know. And we actually got to have sort of like the serious ambitious efforts. We've got to have the like Operation Warp Speed, you know. Um, and I think even if we do the Operation Warp Speed and we have like all the best minds working on it in the world, which I sort of like very much hope we will and I sort of think we will, um, uh, even then I think it's gonna be like a really hard problem. But, um, yeah, I think there's, there's plenty of things we can do. You know, we can, we can look inside their brains. We can have other AIs, um, try to like look inside other AIs brains and like sort of monitor each other and we can, you know, uh, we can try to like study this generalization thing and, you know, things like that. Yeah. That's... Yeah. That is a, that is a helpful message. So you're pitched, uh... I think there's a, you know, there's a pitch here that you can make to other young people, right? It's like you might... I mean, you might save the world. This is, this is ev- this is everything, right? This is the... This is like there's a downside here, right? This could kill us. But there's also... I mean, then this will transition us into the e- about, you know, exponential, uh, economic growth, right? There's a positive here if we get this right. Um, so yeah, let's do that. Let's, let's go into econe- you know, exponential economic growth. What are, what are the prospects of this

## Speaker 2
[40:33] Yeah, yeah. I mean, I mean, one... Well, yeah, one more... Uh, c- you know, maybe a couple more things I'll say on, on... I mean, one is I think sort of... I think alignment is like one of the really big issues. I think it's not the only issue. I mean, I think even if we figure out the, the sort of like alignment part, um, uh, I still think we face basically the like pocket nukes issue. Um, which is like if we have these like very, very powerful systems and maybe as we'll talk about earlier, later, they'll like maybe accelerate science technology a bunch. It's just like, yeah, they'll have like really powerful weapons or like they'll be able to create really destructive things and they'll be very powerful and maybe they'll be able to create really crazy bioweapons and things like that. So I think first order of business is sort of make sure the AIs, um, you know, don't, don't launch a coup against the humans. You know, make sure that sort of the humans can like reliably control the AIs. And even though the AIs can make bioweapons, sort of like they'll only make bioweapons when we tell them to make bioweapons, you know. And so like that's first step. That's the technical problem. I think it's a very important technical problem. It's an unsolved technical problem. All right? So it's like we can't... We can't prevent the AI from just kind of like randomly making a bioweapon directly against us. Once we've solved that technical problem, we still have the te- the problem of like maybe other people will like just tell the AI to make the bioweapon and use it to make a bioweapon. And so I think that... I know, that could be a pretty crazy world. Um, I think there's also just the, the thing about like, look, you don't want Xi Jinping to rule the world. So it's like, look, if we just like... I mean, right now we're ahead, but, you know, maybe we just like fall behind a lot and just, you know, China uses really powerful AIs to take over the world. Um, um, for what it's worth, one, one, one thing that I think is, is, is also really underrated there is just sort of like basic InfoSec. It's like I think people really underrate espionage. China does an extraordinary amount of hacking. I think they've just like infiltrated a lot of our systems. And, um, you know, I think right now sort of the AI labs, you know, sort of the DNA is sort of like, "I'm just a startup." And they very much don't have sort of like nuclear secrets level, uh, InfoSec. And, um, you know, whether you care about... You know, whether you're just like a China hawk and you want to beat China, or you're like one of the AI safety warriors, you're kind of like, "Oh, I don't want China to just kind of like randomly steal the weights of the model." Um, so I think if you're like, you know, looking for things to do... If you're like a government looking for things to do, I'm kind of like, oh man, just like lock down, lock down these like the lab info security. It's just like, oof. Um, you know, get the NSA to do penetration testing, you know, like require security clearances, stuff like that.

## Speaker 1
[42:55] Yeah. Okay. So yeah, so this is, uh, so yeah, this is the, sort of the way we're going. I, you know, the- the pocket nukes issue, you're right. So there's other things to work on. The AI is not a sort of alignment assault and then everything is, you know, happily ever after. We have other issues there. But I wanna, let's talk about, let's talk about the upside. I mean, let's talk about the upside.

## Speaker 2
[43:11] Yeah, yeah, yeah.

## Speaker 1
[43:12] Are we gonna see, you know, what- what are you ... So you have, there's a, there's a report, um, on, uh, from open philanthropy on e- expo- uh, explosive economic growth. You've done a little work on this area. Um, you know, we're- we're- we've stagnated, right? We're down to, you know, two, three percent a year and three percent is a good year now and used to be much higher, you know, uh, several decades ago. Um, and so, you know, one of the, wha- why won't this just sort of continue forever? Are there prospects of us to hyper-charge economic growth again?

## Speaker 2
[43:43] Yeah. Great. So let's talk about this. So, um ... So I think this is again where basically the probability distribution comes in. And I think there's sort of like, you know, it's very plausible basically AI will be sort of another one of, you know, a series of very successful tools, um, that, you know, automate some fraction of the economy and some fraction of what we do, but don't automate everything. Um, and I think this is sort of the strong prior because most technologies have been like this. So even like, um, you know, coding. You know, if you're trying to write a software program right now versus, you know, 50 years ago, um, your coding is a lot more efficient because you have sort of like, you know, various libraries and you can look at other people's code and it's, you know, you're not writing in ones and zeros, you're writing in some programming language. Um, you know, we've like automated lots of farm work, you know, and that's what people used to do. And, um, you know, there used to be sort of big rooms of people, um, uh, doing sort of basic calculations. You know, it's like computers used to be people. Now we have sort of Excel spreadsheets or whatever. And so there's lots of things that are sort of like, you know, they automate a lot or they're like 90% automation. Um, but the remaining 10% is basically always the bottleneck. And that's sort of like the thing that constrains growth. And so even though we have sort of a series of things that automate, you know, 90%, we have that sort of like last 10%, that's the bottleneck, that's what humans do, um, and we sort of get this sort of steady amount of growth. Now maybe AI will be like that and then I think all the sort of like, you know, I think it'll be maybe as big as the internet, you know, it'll be a very big deal. I think, you know, like we're already seeing right now and lots of the tech companies will work on it and, uh, maybe even bo- boost growth a little bit, but the sort of overall picture looks the same. Um, and the overall, I mean, the overall picture th- you know, we've had somewhat slower economic growth, but it's more like, oh, we've gone down from like 3% to 2%. The overall picture is sort of like, you know, the last 150 years or whatever has been, you know, basically constant on like a log chart. So it's just sort of like constant 2% a year economic growth. What could also happen is that the AI, uh, doesn't stop at 90%. It ends up being more like 100% automation. And, um, I'll talk about the plausibility of that in a moment. But I think if you did get 100% automation, that would really, really change things. And in particular, I think that the key thing would be sort of like 100% automation of R&D. Um, and so if you could sort of like automate science and technology, then, you know, the AI could just, you know, come up with much better science and technology and much better inventions. And sort of like science and technology sort of basically is what drives this long run economic growth. And so that could go a lot faster. And so then rather than your reference point being the sort of like last 150 years of very constant growth, if you look at a longer time horizons, like the industrial revolution. The industrial revolution massively accelerated growth. The agriculture revolution. The agriculture revolution massively accelerated growth. So on this sort of like somewhat longer time horizon, uh, overall economic growth has actually been accelerating. Um, and, um, yeah, and so it's just like plausible that AI would do that. Um, and, um, I think, I think basically the scenarios where we should be worried about AI and things might get totally crazy, the thing is, the scenarios where you should be worried about AI, like the AI risk worries or whatever are also the scenarios in which things will get totally crazy, um, because we're gonna get this explosive economic growth. Whereas if it's just, you know, just like the internet or just another 90% technology, you know, we shouldn't be that worried about it

## Speaker 1
[46:58] Yeah. I think you would have to see a really ... I mean, you say, you know, we don't know what AI is going to do to economic growth. But, you know, assuming that it doesn't kill us, the, um, uh, the, uh, you know, the plausibility of it, it's like it's gonna, you know, it's this thing that, you know, just, if there's no advancement, right? You just have ChatGBT, which is like a, you know, much better Google and can write songs for you already, uh, and can, you know, plan your day and, you know, uh, you know, automate, you know, a good portion of your life, that seems, you know, huge already, right?

## Speaker 2
[47:28] Um, right. But I think that's, that's sort of like, you know, internet scale rather than like we're going to go to like 30% a year economic growth or we're going to go to like 100% a year economic growth or sort of like things going really crazy. And I think like I just ... So I think it's very plausible it's only internet scale. I think it's also plausible that things go totally crazy and I think that's the thing people really like underrate. Um, and, um, you know, sort of like in some ways, you know, sometimes people quote sort of like timelines to AGI and there's always like some confusion about what AGI is or whatever and I'm like, "I like to quote timelines to like Dyson spheres." You know, just basically timelines until things are really crazy. Um, because I think if you do get the explosive economic growth world, things could get really, really crazy. Um, yeah. And maybe on that, I think one sort of key, um, key point or like one concrete thing to think about is basically like, um, um, the sort of key milestone would basically be like an AI that can automate every- everything like researchers and engineers at OpenAI do. Um, right? Because if an AI can automate everything researchers and engineers at OpenAI do, then AI can sort of just on computers basically like do what OpenAI researchers do, which is like come up with better algorithms for these AI systems. Um, and, you know, they could come up with better AI systems. And for example, like robotics, people are always like, "Oh, but it won't be able to make sort of scientific discoveries because, you know, it requires experiments in the labs." I was like, "No, it'll solve robotics." Like robotics right now, it's like we have the hardware. The main constraint is just like software, right? It's like, you know, we- we don't have good ways to like make the robots move around in the world. I'm just like, yeah, I kind of think like, um, you know, probably if you had, um, AI f-... OpenAI researchers, they could figure out the software. And in particular, you don't just have sort of like one AI, OpenAI researcher. You have, you know, you'd have sort of like millions of copies and they'd all be running super fast, and probably they'd start getting superhuman. Um, anyway, so that, that sort of benchmark of, like, an AI that can do AI research itself, I think it's, like, really important to emphasize that, like, yeah, I think it's really hard, right? Like, that's quite different from ChatGPT. ChatGPT is, like, a useful tool, and the thing I'm talking about here is, like, a thing that can do, like, 100% of the jobs of the people who work at OpenAI. Um, sort of, like, the value of their labor would go to zero or whatever. Um, but also, you know, again, earlier we were talking about that sort of jump from GPT-2 to GPT-4, where, you know, GPT-2 was sort of like toddler level, could barely count to five, and then, um, you know, GPT-4 was like, you know, 90th percentile at, you know, college and graduate exams. And I'm like, "I don't know. I don't know where this is going."

## Speaker 1
[49:55] What do you think, um ... Do you think that, uh ... What do you think of those arguments that, uh, yeah, the, uh, the internet not having that much, uh, effect on growth is just sort of a product that it's really not capturing the consumer surplus? Imagine a world where, you know, you, you could imagine something similar happening with AI. AI basically takes over all your chores, plans your day, does all the grunt work you don't like. It doesn't cost all that much because anyone can just ... $20 a month or whatever. Uh, but the consumer surplus is huge, and we just use that time to, I don't know, go to the beach and, and lay around. Like, are, are the G- are the GDP numbers even, even capturing things anymore?

## Speaker 2
[50:27] I mean, let's talk about the internet one. Um, what is the ... You know, running water is very cheap. You know, my, uh, my monthly, uh, utility bill, you know, it's, you know, it's not nothing, but it, it's pretty cheap. But, you know, the consumer surplus I get from running water and sewage over, uh, an outhouse and, like, carrying buckets is really fucking big, you know? Um, uh, natural gas, you know, I mean, has been more expensive recently but it's, you know, it's pretty cheap. But, like, the consumer surplus I get from a stove is, is very large, you know? Um, air conditioners. I mean, now electricity is pretty expensive, but it's pretty cheap, but, like, the, the, the consumer surplus from air conditioners are incredible, right? It's just like, you know, entire swaths of the country become basically inhabitable. Um, you know, uh, so I think the point here, and I think this is sort of made well by Robert Gordon in his book The Rise and Fall of American Growth, is that this argument people make about the internet, you could have made just as much, if not honestly probably more, about sort of like the big wave of technologies around sort of the, like, early 20th, mid-20th century. Um-

## Speaker 1
[51:23] But maybe that proves, maybe that proves too much. Maybe that proves that it's, (laughs) it's all nonsense with all the, all the GDP numbers are nonsense, not just the internet, uh, since the internet came along.

## Speaker 2
[51:31] Yeah, I mean, I, I basically think, like, I think G-, all the things people say about how GDP is imperfect is right, um, but I also think it's sort of still underrated. It's a pretty good measure. You know, like, it tracks some real things. Um, and, like, you know, I think, like, it was, you know ... We did grow slower before the Industrial Revolution, and then we did grow more quickly after the Industrial Revolution. And, like, you know, we probably have grown more slowly in the last, you know, whatever, 50 years than in the sort of 50-year period before that. Um, and, you know, some countries go more quickly than others. So yeah, I think, I think there's these issues of sort of, like, long-run sort of GDP measurements. But I think, yeah, I think the thing I'm talking about, if it sort of comes to path, to sort of, like, really path, this, like, really explosive growth, it's sort of like it would be obvious to you. Like, you know, we'd be, um, you know, like, we, we, you know, we'd sort of, like, we'd have, like, little robot factories that are just, like, cranking out just, like, you know, gazillions of, like, little drone swarms, and they all have, like, highly accurate targeting, and there's some, like, crazy WMD war with China going on. And, like, I don't know. Like, you'd notice sort of, but I, I don't, I don't think at that point there's ... I think the, the, the thing I'm saying is, like, will it sort of be roughly on trend for the last, of the last 150 years or will it be, like, a clear break of this, with this trend, sort of like the way the Industrial Revolution was? Um-

## Speaker 1
[52:47] Yeah. That's, that's, uh, that, that's con- that's convincing. Yeah. It's, it's, you're right that there, we are capturing something. It's not, it's not perfect, uh, but it is. Uh, yeah, I think you're right. When I hear one country has a higher GDP than others, it seems to correlate with reality and sort of seems to correlate with the common sense about the Industrial Revolution, uh, and all of that. Um, do you, do you worry that sort of what's, what might happen here is, um, you know, we might strangle the, uh, we might strangle the innovation to death and we won't, we won't be able to basically employ it? So there's a lot of things, like, for example, like, the internet wasn't able to do. Like, um, you know, like, Uber was, like, pretty much illegal but they did it anyway, right? Um, you know, like, uh, uh, education, I think healthcare, a lot of this stuff could be done online, but it's just not. Um, c- you know, I don't know about self-driving cars. They might be, uh ... I've heard some people claim, I don't know if this is true or not, that they're basically safe enough already, they just have to be much safer than human beings or otherwise we'll never, we'll never let them on the road. Um, you know, could it be just, like, humans will just be parasites? They'll organize and they'll protect their, you know, their, their vested interests and then nothing will ever, nothing good will ever happen? Is that, is that, uh, is that a worry of yours?

## Speaker 2
[53:59] I don't know. I mean, look, I think, like, regulation will slow things down. I mean, I don't know. I would, I would ... Let me put it this way. I would have a different attitude towards kind of like Uber regulation if Uber had a 10% chance of, like, literally destroying the world. Um, and I'd be, like, more okay ... Basically, I'm like, yeah, look, I think we should, like ... You know, I definitely think there's, like, huge upsides. Um, I think it'll make people much better off, um, but I'm like, look, if this, like, if we need, like, a couple years here to, like, figure out the alignment thing, let's just, like, do the couple years of the alignment thing.

## Speaker 1
[54:27] Yeah, but, um, I'm, I'm, my question is sort of the opposite. Like, are we gonna overshoot on the alignment thing and we're gonna just not, not have the benefits of AI at all?

## Speaker 2
[54:34] I don't think we're gonna overshoot on the alignment thing. I think we might, like, have all sorts of other dumb regulation. Like, we'll be like... You know. And I think you're already kind of seeing this right now, you know. The sort of current regulation push isn't necessarily translating into like, they're working on the right thing. I mean, that's why in general I think I'm, like, a little bit less enthusiastic, at least right now, about these sort of pushes for pauses and, you know, government projects and things like that. I'm kind of like, "Look. I don't know, man." Like, maybe you don't like Sam Altman that much or whatever, but man, the guy is pretty smart, you know. And like, Dario's pretty smart and Demis is pretty smart. And they're like, pretty, they're pretty on it, you know. Like, I mean, these people are, if anything, are like the most alarmed about the AI extras thing. Um, and, um, so I'm like, I think that's probably better than, you know, Kamala Harris. Um, I do think, like, um, I don't know. I just, I also think before we kind of... Yeah. So, I, I think probably we'll have all sorts of like kludgy regulation and it'll be imperfect. Um, I think probably on the margin, like, people still won't be, like, doing enough on the alignment thing. So, like, on the margin, I really think we should be, like, pushing on the alignment thing. And then, like, yeah. I don't know. Like, maybe we'll delay AI a bit. I think it's like, I think the real down-... I think the real worry there isn't kind of like, "Oh, we'll delay the big upside by a couple years." I think the main thing that matters is like, where are we at in like 50 years or whatever. I think the worry there is more like, oh, maybe, like, there's bad actors with AIs. Like, I don't know, just like China. And so I think that's mostly the constraint on kind of like your, like, regulation or whatever, which is like, "Look, China is a few years behind you. Uh, if this, like, explosive economic growth thing is right, like if China gets AGI first, they could just, like, totally beat you and, like, take over the world." And so, you know, you can't, you don't have sort of like infinite runway there. And so you have some sort of like limited budget of, um, of, of, you know, of sort of like, you know, a little bit of like lead to burn or whatever. And just like, yeah, use that really wisely. Don't just kind of like burn that on dumb stuff. Um, yeah. And I, I mean, look, I just think the main thing, before people kind of... I feel like with COVID, we kind of ended up getting... You know, I wrote a blog post about this, you know. Uh, with COVID, we ended up getting into these, like, huge fights about, you know, lockdown or not, and everyone's like, "Lockdown or not?" And the schools or whatever, and it's like, just forget about the lockdown or not. Just, like, make the vaccines. And that was, like, just the clear solution to the COVID thing.

## Speaker 1
[56:31] But that's, that's- Yeah. But that's not clear to everyone. That is very controversial now. A lot of people hate the vaccine and they, they, you know. So it's like, yeah you're right.

## Speaker 2
[56:38] Yeah, yeah, yeah. I mean, I think you... Yeah. Well, uh, you know, I think those people are wrong, you know. It's just like, "Look, make the vaccine." Just like Operation Warp Speed. That was clearly the way out. Just like Operation Warp Speed plus like, you know, a bit of masking in the mid, in the middle or whatever. And I'm just like, "Look, on AI, let's, let's, like, let's do the Operation Warp Speed for AI alignment." You know, it's like, "Look, Demis, you're, like, really concerned about AI extras? Like, put your, like, top ML researchers on this." You know, I just think there's like, we can just, like... Much better things are possible here. And sort of before we get into, like, the endless online debates or the sort of, the sort of regulatory things, I'm just like, "Let's actually just, like, try to solve this problem." Um, yeah.

## Speaker 1
[57:10] Yeah. One thing we talked about at the EV conference, uh, Emerging Ventures Conference, um, is that, uh, yeah. I mean, there's no, there's no permanent pause on this, on this stuff, right? Like, do we have... Like, you know, it might as well happen now, and it might as well happen here. And if we're all gonna die, we're all gonna die. But like, you know, if there is some, like, human threshold where we have... You know, it's, there's no, there's, I don't think there's an answer where we just, you know, become Luddites. We just, you know, cut off humanity from, from its future, just say, "We're gonna, you know, quash this stuff, uh, forever." Or is there? Did you... By the way, did you see the China, uh, the, there was, there was... You talked about China a few times. There was news out of China that basically they're, they're really, like, taking a heavy hand to, uh, regulations. And my, my, my, um, my sort of idea of sort of the, um, the Chinese government, um, in watching COVID as like this is, is they are very sort of risk averse. They don't like things that destabilize society. So, it doesn't seem to me to be cr- to seem crazy that they would actually want to slow down AI research or just, you know, sort of com- uh, quash it. And, you know, they're good at sort of, you know, they're good at quashing things that they don't, that they don't like. Um, so maybe, yeah. I started this by saying-

## Speaker 2
[58:20] Yeah, I mean-

## Speaker 1
[58:21] ... there's no hope, but, but maybe there is a hope of just, just shutting this stuff down for a long time.

## Speaker 2
[58:25] No. I mean, I, I... Definitely not forever. I mean, it just... The other thing is just there's extremely rapid just algorithmic progress, right? So, sort of like GP3 maybe at some point cost like 10 plus million to, to train, and now you can sort of like just train it on your own for a few hundred thousand dollars. Um, and in general, sort of there's like, you know, roughly half an order of magnitude a year of algorithms becoming more efficient. And so people are like, "Oh, we'll regulate the compute clusters." I'm like, "Yes, that gives you a few years." And so I think all of, all of your things are like, "Look, we have a few years of lead. Can we, like, use those few years of lead to do all our safety things?" But then, you know, like if you wait long enough, it's just like, it becomes real-... Like, you know, you get the algorithmic progress. It becomes much easier to train. You know, you get open source, you get China. Um, and, um, look, maybe there's, like, extreme worlds th- where that becomes necessary. I mean, I think... And one thing in general is I think we're hopefully gonna get just a lot more empirical evidence on this sort of alignment thing. So, you know, people working on empirical test beds of deception. Like, is this AI system, just like, does it deceive humans? And like, people at Anthropic are working on that, and I think that'll be really useful. People working on these sort of like evaluations of, you know, basically, like, will it make bio weapons and will it try to, like, autonomously spread over the internet? Um, you know, basic reward hacking demos. And so I'm very much into like, "Look, I think we should, like, do these evals and we'll get empirical evidence on this, and maybe it'll be fine." Or like, "Oh shit, this thing will be really scary." And, um, anyway. So, who... And maybe, maybe there are worlds where it's really scary and everyone realizes and it's immediately obvious, and maybe we'll do other things then. But I think sort of by default, I would expect we have, we have a few years lead or whatever. You know, we have the export controls on China. Those are very important. They can't get the most leading edge chips, and they've all cr- all run, you know... All throughout this supply chain, we've cut them off. Hmm. You can't shut them down forever. But, you know, we have a bit of time, um, and like I said, it's like, I think this, like, alignment thing is totally real and we can just, like, totally screw it up. And like, if we screw it up, it's just like, yeah. Totally the AIs could take over. Um, but, um, it's also like there's a lot of things we could do. And so I think there's this sort of... Yeah. I think it's actually just quite tractable. Um, and I really want us to sort of do those alignment investments. Um, and I think people are really fatalistic, you know. "Oh, it's like..." You know. Like, no, no. There's just like tons of technical stuff we can do. There's sort of like all the smart people are now becoming interested. Um, there's like tons of money who is potentially interested in funding it. Like, let's do it. Let's make it happen

## Speaker 1
[1:00:39] Yeah.

## Speaker 2
[1:00:39] Um, you know. (laughs)

## Speaker 1
[1:00:41] (laughs) Okay. I love, I love the message. I love the optimism. I love the idea that we don't have to be hopeless or we don't have to be blase. We can... You know, there's, there's, there's something we can do here, and I think you've done a good job of sort of inspiring people, you know, on that account. And I think y- you here will, will do that too. Um, the, uh... Okay. Yeah. This was, this was great. Is there, uh... You know, what are you working on no- uh, now, Leopold? Like, are you doing more AI stuff? What's sort of, what's sort of, uh, next steps for you?

## Speaker 2
[1:01:07] Yeah. Doing more AI stuff. Um, I think, um, you know, I've sort of spoken a bit about, uh, you know, I think we should have more ambitious concerted alignment efforts and, you know, hopefully we'll have more to share on that at some point. But yeah. That's what I think we should do. You know, I think we should do the Operation Warp Speed.

## Speaker 1
[1:01:21] Sounds good. All right, man. Good talking to you.

## Speaker 2
[1:01:24] Good talking. (instrumental music)
